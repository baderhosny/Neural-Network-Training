<img width="253" alt="image" src="https://github.com/baderhosny/Neural-Network-Training/assets/60364067/ba4470b7-9450-412d-b8cb-d690d6a27d22">Hello,

This project was worked on during my Master's in Computer Engineering, I have designed a neural network with 2 hidden layers.
2 classes with features, we have trained the model on multiple neuron count steps, for example with 20 neurons we have seen of getting a 7.4% missed rate in performance.
The equation that we have used is => (# of times prediction != target) / number of inputs

As we have implemented more layers into our network to see how the performance would change, we canâ€™t see a dramatic change between task 1.2 and task 1.3,
changing as we increase the number of layers, this might be causing some impact on the learning rate, but not in a huge manner

Looking at our stimulation and comparing some of it to our old project we can see that an approximation function
can play a huge role in teaching the network, although we can also still see that similar network algorithms with different function 
sometimes would yield to similar behavior of output, given that our network is small and not complex we can see that there would be a massive change
when we are working with networks with hundreds of hidden layers and parameters.
